{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedghazihaider/Machine-learning-final-project/blob/main/Copy_of_MLPROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Project title = Customer Churn and Behavior Analysis\n",
        "#Name = Syed Ghazi Haider\n",
        "#Roll number = 24k-7314"
      ],
      "metadata": {
        "id": "-xf2fucHPzp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "p9p1L_sKivWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Target: 1 if Recency > 90 days (churned), else 0\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from prophet import Prophet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "import networkx as nx\n",
        "from prophet.plot import plot_plotly\n",
        "import plotly.offline as py\n",
        "import shap\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "from prophet.plot import plot_cross_validation_metric\n",
        "from sklearn.model_selection import TimeSeriesSplit"
      ],
      "metadata": {
        "id": "o8bJDoUciwTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"Online Retail.xlsx\")\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z4bvguqNkUUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove canceled orders (InvoiceNo starting with 'C')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.contains('C')]\n",
        "\n",
        "# Drop rows with missing CustomerID\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "\n",
        "# Convert CustomerID to integer\n",
        "df['CustomerID'] = df['CustomerID'].astype(int)\n",
        "\n",
        "# Add TotalPrice column\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']"
      ],
      "metadata": {
        "id": "bHpN2Lf0kb9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set snapshot date (last invoice date + 1 day)\n",
        "snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "\n",
        "# Calculate RFM metrics\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,  # Recency\n",
        "    'InvoiceNo': 'nunique',                                  # Frequency\n",
        "    'TotalPrice': 'sum'                                      # Monetary\n",
        "}).rename(columns={\n",
        "    'InvoiceDate': 'Recency',\n",
        "    'InvoiceNo': 'Frequency',\n",
        "    'TotalPrice': 'Monetary'\n",
        "})"
      ],
      "metadata": {
        "id": "-bGgJk5MoSFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log-transform to handle skewness\n",
        "rfm_log = rfm[['Recency', 'Frequency', 'Monetary']].apply(np.log1p)\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm_log)\n",
        "\n",
        "# Elbow method to find optimal clusters\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(rfm_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.show()\n",
        "\n",
        "# Apply K-Means (assuming 4 clusters)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "USEcGg9ioXwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster analysis\n",
        "cluster_summary = rfm.groupby('Cluster').agg({\n",
        "    'Recency': 'mean',\n",
        "    'Frequency': 'mean',\n",
        "    'Monetary': ['mean', 'count']\n",
        "}).round(1)\n",
        "\n",
        "# Plot clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='Frequency',\n",
        "    y='Monetary',\n",
        "    hue='Cluster',\n",
        "    data=rfm,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.title('Customer Segments by Frequency vs Monetary Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-RFnjsx9ofAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Cohort Analysis (Fixed)\n",
        "# Create cohort based on first purchase month\n",
        "df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
        "df['Cohort'] = df.groupby('CustomerID')['InvoiceMonth'].transform('min')\n",
        "\n",
        "# Calculate retention metrics\n",
        "cohort_data = df.groupby(['Cohort', 'InvoiceMonth']).agg(\n",
        "    n_customers=('CustomerID', 'nunique')\n",
        ").reset_index()\n",
        "cohort_data['PeriodNumber'] = (cohort_data['InvoiceMonth'] - cohort_data['Cohort']).apply(\n",
        "    lambda x: x.n)\n",
        "\n",
        "# Pivot for retention matrix\n",
        "cohort_pivot = cohort_data.pivot_table(\n",
        "    index='Cohort',\n",
        "    columns='PeriodNumber',\n",
        "    values='n_customers',\n",
        "    aggfunc='sum'\n",
        ")\n",
        "\n",
        "# Calculate retention rates\n",
        "cohort_size = cohort_pivot.iloc[:, 0]\n",
        "retention_matrix = cohort_pivot.divide(cohort_size, axis=0)\n",
        "\n",
        "# Visualize\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(retention_matrix,\n",
        "            annot=True,\n",
        "            fmt='.0%',\n",
        "            cmap='Blues',\n",
        "            vmin=0.0,\n",
        "            vmax=0.5)\n",
        "plt.title('Monthly Cohorts: Customer Retention Rates')\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u7KS5uJeuwSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert Cohort to string for plotting\n",
        "cohort_clv['Cohort_Str'] = cohort_clv['Cohort'].astype(str)\n",
        "\n",
        "# 2. Enhanced Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.lineplot(\n",
        "    data=cohort_clv,\n",
        "    x='Cohort_Str',\n",
        "    y='avg_purchase',\n",
        "    marker='o',\n",
        "    linewidth=2.5,\n",
        "    color='#4B8BBE'\n",
        ")\n",
        "\n",
        "# 3. Professional Formatting\n",
        "plt.title('Average Purchase Value by Cohort', pad=20, fontsize=14)\n",
        "plt.xlabel('Cohort Month', labelpad=10)\n",
        "plt.ylabel('Average Purchase ($)', labelpad=10)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# 4. Add Value Annotations\n",
        "for x, y in zip(cohort_clv.index, cohort_clv['avg_purchase']):\n",
        "    ax.text(x, y+0.5, f'${y:.2f}',\n",
        "            ha='center',\n",
        "            va='bottom',\n",
        "            fontsize=9)\n",
        "\n",
        "# 5. Add Business Insights\n",
        "max_cohort = cohort_clv.loc[cohort_clv['avg_purchase'].idxmax()]\n",
        "plt.axvline(x=max_cohort.name,\n",
        "            color='red',\n",
        "            linestyle='--',\n",
        "            alpha=0.3)\n",
        "plt.text(max_cohort.name+0.3, max_cohort['avg_purchase']-2,\n",
        "         f\"Peak: {max_cohort['Cohort_Str']}\\n${max_cohort['avg_purchase']:.2f}\",\n",
        "         color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mivq3pXju07e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Cohort to ordinal numbers\n",
        "cohort_clv['Cohort_Num'] = range(len(cohort_clv))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=cohort_clv, x='Cohort_Num', y='avg_purchase', marker='o')\n",
        "plt.xticks(ticks=cohort_clv.index,\n",
        "           labels=cohort_clv['Cohort_Str'],\n",
        "           rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z4_Ckiz2vTpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cohort Analysis (Retention Trends)\n",
        "df['InvoicePeriod'] = df['InvoiceDate'].dt.to_period('M')\n",
        "cohorts = df.groupby(['Cohort', 'InvoicePeriod']).agg(n_customers=('CustomerID', 'nunique')).reset_index()\n",
        "\n",
        "# 2. Customer Lifetime Value (CLV)\n",
        "clv = rfm['Monetary'].mean() * (rfm['Frequency'].mean() / (1 - (1 / rfm['Recency'].mean())))\n",
        "print(f\"Average CLV: ${clv:.2f}\")"
      ],
      "metadata": {
        "id": "yG1f0DsvopzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Feature Engineering\n",
        "rfm['AvgPurchaseValue'] = rfm['Monetary'] / rfm['Frequency']\n",
        "rfm['PurchaseCadence'] = rfm['Recency'] / rfm['Frequency']\n",
        "rfm['Churned'] = (rfm['Recency'] > 90).astype(int)  # Target variable\n",
        "\n",
        "# Prepare data\n",
        "X = rfm[['Recency', 'Frequency', 'Monetary', 'AvgPurchaseValue', 'PurchaseCadence']]\n",
        "y = rfm['Churned']\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y  # Maintain class balance\n",
        ")\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',  # Focus on F1-score for imbalanced data\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model evaluation\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"=== Best Parameters ===\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\n=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b5J1CxmfpVDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Prepare Transaction Data\n",
        "transactions = df.groupby('InvoiceNo')['Description'].apply(list).values.tolist()\n",
        "\n",
        "# 2. One-Hot Encode Transactions\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "basket_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "# 3. Generate Frequent Itemsets\n",
        "frequent_itemsets = apriori(\n",
        "    basket_df,\n",
        "    min_support=0.03,  # Items appear in 3% of transactions\n",
        "    use_colnames=True,\n",
        "    max_len=3  # Max itemset size\n",
        ")\n",
        "\n",
        "# 4. Generate Association Rules\n",
        "rules = association_rules(\n",
        "    frequent_itemsets,\n",
        "    metric=\"lift\",\n",
        "    min_threshold=1.5  # At least 50% better than random\n",
        ")\n",
        "\n",
        "# 5. Filter Strong Rules\n",
        "high_lift_rules = rules[\n",
        "    (rules['lift'] > 2) &\n",
        "    (rules['confidence'] > 0.6) &\n",
        "    (rules['support'] > 0.04)\n",
        "].sort_values('lift', ascending=False)\n",
        "\n",
        "# 6. Visualize Top 10 Rules\n",
        "plt.figure(figsize=(12, 6))\n",
        "G = nx.from_pandas_edgelist(\n",
        "    high_lift_rules.head(10),\n",
        "    'antecedents',\n",
        "    'consequents',\n",
        "    edge_attr=['lift', 'confidence']\n",
        ")\n",
        "\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "nx.draw(\n",
        "    G, pos,\n",
        "    with_labels=True,\n",
        "    node_size=2500,\n",
        "    node_color='skyblue',\n",
        "    font_size=10,\n",
        "    width=[d['lift']*0.5 for (u,v,d) in G.edges(data=True)]\n",
        ")\n",
        "edge_labels = {(u,v): f\"Lift: {d['lift']:.2f}\\nConf: {d['confidence']:.2f}\"\n",
        "               for (u,v,d) in G.edges(data=True)}\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "\n",
        "plt.title(\"Top Product Associations (Lift > 2, Confidence > 60%)\", pad=20)\n",
        "plt.show()\n",
        "\n",
        "# 7. Print Actionable Rules\n",
        "print(\"=== Top 5 Actionable Bundles ===\")\n",
        "for idx, row in high_lift_rules.head(5).iterrows():\n",
        "    print(f\"\\nBundle {idx+1}:\")\n",
        "    print(f\"• Buy {', '.join(list(row['antecedents']))}\")\n",
        "    print(f\"• Then buy: {', '.join(list(row['consequents']))}\")\n",
        "    print(f\"► Confidence: {row['confidence']:.0%}\")\n",
        "    print(f\"► Lift: {row['lift']:.1f}x improvement over random\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wVUrenLdwP4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Prepare UK Holidays DataFrame (Manual Example)\n",
        "uk_holidays_df = pd.DataFrame({\n",
        "    'holiday': 'uk_holiday',\n",
        "    'ds': pd.to_datetime([\n",
        "        '2025-01-01',  # New Year's Day\n",
        "        '2025-04-18',   # Good Friday\n",
        "        '2025-04-21',   # Easter Monday\n",
        "        '2025-05-05',   # Early May Bank Holiday\n",
        "        '2025-05-26',   # Spring Bank Holiday\n",
        "        '2025-08-25',   # Summer Bank Holiday\n",
        "        '2025-12-25',   # Christmas Day\n",
        "        '2025-12-26'    # Boxing Day\n",
        "    ]),\n",
        "    'lower_window': -2,  # 2 days before holiday\n",
        "    'upper_window': 1     # 1 day after holiday\n",
        "})\n",
        "\n",
        "# 2. Prepare Sales Data (Daily Aggregation)\n",
        "daily_sales = df.resample('D', on='InvoiceDate')['TotalPrice'].sum().reset_index()\n",
        "daily_sales.columns = ['ds', 'y']\n",
        "\n",
        "# 3. Initialize and Fit Model\n",
        "model = Prophet(\n",
        "    holidays=uk_holidays_df,\n",
        "    seasonality_mode='multiplicative',\n",
        "    yearly_seasonality=8,\n",
        "    weekly_seasonality=3\n",
        ")\n",
        "\n",
        "# Optional: Add Marketing Spend Regressor (if available)\n",
        "# model.add_regressor('marketing_spend')\n",
        "\n",
        "model.fit(daily_sales)\n",
        "\n",
        "# 4. Make Future DataFrame (6 months forecast)\n",
        "future = model.make_future_dataframe(periods=180)\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# 5. Visualize Results\n",
        "fig1 = model.plot(forecast)\n",
        "plt.title(\"Daily Sales Forecast with UK Holidays\", pad=20)\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales (£)\")\n",
        "plt.show()\n",
        "\n",
        "# Interactive Plot (Zoomable)\n",
        "fig2 = plot_plotly(model, forecast)\n",
        "py.iplot(fig2)\n",
        "\n",
        "# 6. Show Holiday Impacts\n",
        "print(\"=== Top Holiday Impacts ===\")\n",
        "holiday_effects = forecast[\n",
        "    (forecast['uk_holiday'] != 0) &\n",
        "    (forecast['ds'].dt.year == 2025)\n",
        "][['ds', 'uk_holiday']].drop_duplicates()\n",
        "print(holiday_effects.sort_values('uk_holiday', ascending=False))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5px-Lok31FUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "explainer = shap.TreeExplainer(grid_search.best_estimator_)\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "metadata": {
        "id": "T88PC5kX1dpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Calculate safe CV parameters\n",
        "n_hist = len(daily_sales)\n",
        "min_days = 90  # Minimum training days\n",
        "horizon_days = 30  # Forecast window\n",
        "cv_days = 60  # Step between cutoff dates\n",
        "\n",
        "# 2. Run Cross-Validation (with error handling)\n",
        "try:\n",
        "    df_cv = cross_validation(\n",
        "        model,\n",
        "        initial=f'{min_days} days',\n",
        "        horizon=f'{horizon_days} days',\n",
        "        period=f'{cv_days} days'\n",
        "    )\n",
        "\n",
        "    # 3. Calculate performance metrics\n",
        "    df_p = performance_metrics(df_cv)\n",
        "    print(\"=== Cross-Validation Metrics ===\")\n",
        "    print(df_p.head())\n",
        "\n",
        "    # 4. Plot MAE over horizon\n",
        "    fig = plot_cross_validation_metric(df_cv, metric='mae')\n",
        "    plt.title(f'MAE Over {horizon_days}-Day Forecast Horizon')\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\\nTry reducing horizon/initial parameters.\")\n",
        "    print(f\"Current data: {n_hist} days | Min required: {min_days + horizon_days} days\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u4voAduw1gXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "for train_idx, test_idx in tscv.split(daily_sales):\n",
        "    train = daily_sales.iloc[train_idx]\n",
        "    test = daily_sales.iloc[test_idx]\n",
        "\n",
        "    # Train and evaluate on each fold\n",
        "    model = Prophet().fit(train)\n",
        "    forecast = model.predict(test)\n",
        "    # Calculate metrics..."
      ],
      "metadata": {
        "collapsed": true,
        "id": "ul4Swqsb15MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVLLJNKJvyUf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}